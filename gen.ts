// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// code-gen.ts â€” Supabase Schema-Driven Code Generator v2
// Generates: {table}-sdk.ts, {table}-types.ts, {table}.sql
// Patterns: vector search, JSONB merge/append/stats, array ops, queue triggers
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import * as yaml from 'yaml'
import * as fs from 'fs'
import * as path from 'path'

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Schema Interfaces â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

interface SchemaColumn {
  name: string; type: string; primary?: boolean; unique?: boolean;
  nullable?: boolean; default?: unknown; patterns?: string[];
  embed_source?: string; index?: string;
  reference?: string | ColumnReference;
  references?: string | ColumnReference;
}

interface TableSchema {
  description?: string; columns: SchemaColumn[];
  access?: 'anon' | 'authenticated' | Array<'anon' | 'authenticated'>;
  features?: string[]; indexes?: { columns: string[]; type: string; ops?: string }[];
}

interface Schema {
  tables: Record<string, TableSchema>
  config?: { output_dir?: string; embed_model?: string; embed_dim?: number; db_schema?: string }
}

interface ColumnReference {
  table: string;
  column?: string;
  schema?: string;
  on_delete?: string;
  on_update?: string;
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Analysis Interfaces â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

interface ResolvedColumn extends SchemaColumn {
  resolvedPatterns: string[]; tsType: string;
  isOptionalInsert: boolean; isAutoGenerated: boolean;
}

interface TableAnalysis {
  name: string; pascal: string; primary: ResolvedColumn;
  columns: ResolvedColumn[];
  vectorCols: ResolvedColumn[]; jsonbCols: ResolvedColumn[]; arrayCols: ResolvedColumn[];
  embedMap: Map<string, string>;       // vectorCol â†’ sourceCol
  searchVectorCol: ResolvedColumn | null;
  mergeCols: ResolvedColumn[]; appendCols: ResolvedColumn[]; statsCols: ResolvedColumn[];
  contentCols: ResolvedColumn[];       // trigger re-enqueue on change
  accessRoles: Array<'anon' | 'authenticated'>;
  hasEmbedder: boolean; hasQueueTrigger: boolean; hasSearch: boolean;
}

const VECTOR_TYPE_FQN = 'extensions.vector'
const VECTOR_COSINE_OPS_FQN = 'extensions.vector_cosine_ops'

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Naming Utilities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const toPascal = (s: string): string =>
  s.split('_').map(w => w.charAt(0).toUpperCase() + w.slice(1)).join('')

const toCamel = (s: string): string => {
  const p = toPascal(s)
  return p.charAt(0).toLowerCase() + p.slice(1)
}

const singularize = (s: string): string => {
  if (s.endsWith('ies')) return s.slice(0, -3) + 'y'
  if (s.endsWith('ses') || s.endsWith('xes') || s.endsWith('zes')) return s.slice(0, -2)
  if (s.endsWith('s') && !s.endsWith('ss') && !s.endsWith('us')) return s.slice(0, -1)
  return s
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Type Mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/** Postgres â†’ TypeScript type map */
const mapTsType = (pgType: string): string => {
  const t = pgType.toLowerCase().replace(/\s+/g, '')
  if (t.startsWith('vector')) return 'number[]'
  if (t.endsWith('[]')) return `${mapTsType(t.slice(0, -2))}[]`
  if (['uuid', 'text', 'varchar'].includes(t) || t.startsWith('char')) return 'string'
  if (['timestamptz', 'timestamp', 'date', 'time'].includes(t)) return 'string'
  if (['int', 'integer', 'bigint', 'smallint', 'serial', 'bigserial'].includes(t)) return 'number'
  if (t.startsWith('numeric') || ['float', 'float4', 'float8', 'real', 'doubleprecision'].includes(t)) return 'number'
  if (['boolean', 'bool'].includes(t)) return 'boolean'
  if (['jsonb', 'json'].includes(t)) return 'Record<string, any>'
  return 'any'
}

/** Primary key type â†’ TypeScript for method signatures */
const mapIdType = (pgType: string): string => {
  const t = pgType.toLowerCase()
  if (['int', 'integer', 'bigint', 'smallint', 'serial', 'bigserial'].includes(t)) return 'number'
  return 'string'
}

const toSqlType = (pgType: string): string => {
  const t = pgType.trim()
  if (/^vector\s*\(/i.test(t)) return t.replace(/^vector/i, VECTOR_TYPE_FQN)
  if (/^vector$/i.test(t)) return VECTOR_TYPE_FQN
  return t
}

const toSqlOps = (ops?: string): string | undefined => {
  if (!ops) return ops
  return ops.replace(/\bvector_cosine_ops\b/g, VECTOR_COSINE_OPS_FQN)
}

const normalizeDbSchema = (schema?: string): string => {
  const value = (schema ?? 'public').trim()
  if (!/^[a-zA-Z_][a-zA-Z0-9_]*$/.test(value)) {
    throw new Error(`Invalid db schema "${schema}"`)
  }
  return value
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Pattern Resolution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Resolves column patterns: explicit first, then auto-detect from type + name.
 * - vector   â†’ similarity_search
 * - jsonb    â†’ jsonb_merge (default), jsonb_append (trace-like), jsonb_stats (metrics-like)
 * - array    â†’ array_contains, array_overlap
 */
function resolvePatterns(col: SchemaColumn): string[] {
  if (col.patterns?.length) return col.patterns

  const t = col.type.toLowerCase()
  if (t.startsWith('vector')) return ['similarity_search']
  if (t.endsWith('[]')) return ['array_contains', 'array_overlap']
  if (t === 'jsonb' || t === 'json') {
    if (/trace|log|history|events/i.test(col.name)) return ['jsonb_append']
    if (/metrics|scores|stats/i.test(col.name)) return ['jsonb_merge', 'jsonb_stats']
    return ['jsonb_merge']
  }
  return []
}

function normalizeAccessRoles(input: TableSchema['access']): Array<'anon' | 'authenticated'> {
  if (!input) return []
  const raw = Array.isArray(input) ? input : [input]
  const unique = Array.from(new Set(raw))
  const invalid = unique.filter(r => r !== 'anon' && r !== 'authenticated')
  if (invalid.length) {
    throw new Error(`Invalid access role(s): ${invalid.join(', ')}. Allowed values: anon, authenticated`)
  }
  return unique as Array<'anon' | 'authenticated'>
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Table Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function analyzeTable(name: string, table: TableSchema): TableAnalysis {
  const columns: ResolvedColumn[] = table.columns.map(col => {
    const t = col.type.toLowerCase()
    const isPrimary = col.primary === true
    const hasDefault = !!col.default
    const isTimestamp = ['timestamptz', 'timestamp', 'date'].includes(t)
    const isAutoGen = (isPrimary && hasDefault) || !!col.embed_source || (isTimestamp && hasDefault)

    return {
      ...col,
      resolvedPatterns: resolvePatterns(col),
      tsType: mapTsType(col.type),
      isOptionalInsert: isAutoGen || col.nullable === true || hasDefault,
      isAutoGenerated: isAutoGen,
    }
  })

  const primary = columns.find(c => c.primary)
  if (!primary) throw new Error(`Table "${name}" has no primary key column`)

  const vectorCols = columns.filter(c => c.type.toLowerCase().startsWith('vector'))
  const jsonbCols = columns.filter(c => ['jsonb', 'json'].includes(c.type.toLowerCase()))
  const arrayCols = columns.filter(c => c.type.endsWith('[]'))

  // Build embed source map: vectorCol â†’ sourceCol
  const embedMap = new Map<string, string>()
  for (const vc of vectorCols) {
    if (vc.embed_source) embedMap.set(vc.name, vc.embed_source)
  }

  // Pick primary search vector: prefer one with embed_source
  const searchVectorCol = vectorCols.find(c => c.embed_source) ?? vectorCols[0] ?? null

  const mergeCols = jsonbCols.filter(c => c.resolvedPatterns.includes('jsonb_merge'))
  const appendCols = jsonbCols.filter(c => c.resolvedPatterns.includes('jsonb_append'))
  const statsCols = jsonbCols.filter(c => c.resolvedPatterns.includes('jsonb_stats'))

  // Content columns: trigger re-enqueue only when these change
  // Excludes: primary, vector, jsonb, timestamps
  const contentCols = columns.filter(c =>
    !c.primary && !c.embed_source &&
    !c.type.toLowerCase().startsWith('vector') &&
    !['jsonb', 'json', 'timestamptz', 'timestamp'].includes(c.type.toLowerCase())
  )

  const features = table.features ?? []
  const accessRoles = normalizeAccessRoles(table.access)

  return {
    name, pascal: toPascal(singularize(name)),
    primary, columns, vectorCols, jsonbCols, arrayCols,
    embedMap, searchVectorCol, mergeCols, appendCols, statsCols, contentCols,
    accessRoles,
    hasEmbedder: vectorCols.length > 0,
    hasQueueTrigger: features.includes('queue_trigger'),
    hasSearch: searchVectorCol !== null,
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ğŸ—„ï¸  SQL GENERATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function normalizeRefAction(action?: string): string | undefined {
  if (!action) return undefined
  const normalized = action.trim().toUpperCase().replace(/\s+/g, ' ')
  const allowed = new Set(['CASCADE', 'RESTRICT', 'NO ACTION', 'SET NULL', 'SET DEFAULT'])
  if (!allowed.has(normalized)) {
    throw new Error(`Unsupported foreign key action "${action}"`)
  }
  return normalized
}

function parseReferenceText(rawRef: string): ColumnReference {
  const raw = rawRef.trim()
  let m = raw.match(/^([a-zA-Z_]\w*)\.([a-zA-Z_]\w*)\(([a-zA-Z_]\w*)\)$/)
  if (m) return { schema: m[1], table: m[2], column: m[3] }

  m = raw.match(/^([a-zA-Z_]\w*)\(([a-zA-Z_]\w*)\)$/)
  if (m) return { table: m[1], column: m[2] }

  m = raw.match(/^([a-zA-Z_]\w*)\.([a-zA-Z_]\w*)\.([a-zA-Z_]\w*)$/)
  if (m) return { schema: m[1], table: m[2], column: m[3] }

  m = raw.match(/^([a-zA-Z_]\w*)\.([a-zA-Z_]\w*)$/)
  if (m) return { table: m[1], column: m[2] }

  m = raw.match(/^([a-zA-Z_]\w*)$/)
  if (m) return { table: m[1] }

  throw new Error(`Invalid references format "${rawRef}"`)
}

function inferReferenceFromColumn(col: SchemaColumn, knownTables: string[]): ColumnReference | null {
  if (col.name === 'id' || !col.name.endsWith('_id')) return null
  const base = col.name.slice(0, -3)
  const candidates = new Set<string>([
    base,
    singularize(base),
    `${base}s`,
    `${base}es`,
    base.endsWith('y') ? `${base.slice(0, -1)}ies` : base
  ])
  for (const c of candidates) {
    if (knownTables.includes(c)) return { table: c, column: 'id' }
  }
  return null
}

function resolveColumnReference(col: SchemaColumn, knownTables: string[]): ColumnReference | null {
  const raw = col.references ?? col.reference
  if (!raw) return inferReferenceFromColumn(col, knownTables)

  const parsed = typeof raw === 'string' ? parseReferenceText(raw) : raw
  return {
    schema: parsed.schema,
    table: parsed.table,
    column: parsed.column ?? 'id',
    on_delete: normalizeRefAction(parsed.on_delete),
    on_update: normalizeRefAction(parsed.on_update),
  }
}

function sqlColumnDef(col: SchemaColumn, knownTables: string[], dbSchema: string): string {
  const defaultExpr = sqlDefaultExpr(col)
  const ref = resolveColumnReference(col, knownTables)
  let def = `  ${col.name} ${toSqlType(col.type)}`
  if (col.primary) def += ' PRIMARY KEY'
  if (col.unique) def += ' UNIQUE'
  if (col.nullable === false && !col.primary) def += ' NOT NULL'
  if (defaultExpr) def += ` DEFAULT ${defaultExpr}`
  if (ref) {
    def += ` REFERENCES ${(ref.schema ?? dbSchema)}.${ref.table}(${ref.column ?? 'id'})`
    if (ref.on_delete) def += ` ON DELETE ${ref.on_delete}`
    if (ref.on_update) def += ` ON UPDATE ${ref.on_update}`
  }
  return def
}

function sqlDefaultExpr(col: SchemaColumn): string | null {
  if (col.default === undefined || col.default === null) return null

  const t = col.type.toLowerCase().replace(/\s+/g, '')
  const raw = typeof col.default === 'string' ? col.default.trim() : JSON.stringify(col.default)

  if ((t === 'jsonb' || t === 'json') && (raw === '{}' || raw === '[]')) {
    return `'${raw}'::${t}`
  }

  return raw
}

function genTableDDL(name: string, table: TableSchema, knownTables: string[], dbSchema: string): string {
  const defs = table.columns.map(c => sqlColumnDef(c, knownTables, dbSchema)).join(',\n')
  return `CREATE TABLE IF NOT EXISTS ${dbSchema}.${name} (\n${defs}\n);`
}

function tableUsesDefaultFn(table: TableSchema, fnName: string): boolean {
  const pattern = new RegExp(`\\b${fnName}\\s*\\(`, 'i')
  return table.columns.some(col => typeof col.default === 'string' && pattern.test(col.default))
}

function genIndexes(name: string, table: TableSchema, a: TableAnalysis, dbSchema: string): string {
  const lines: string[] = []
  const declared = new Set((table.indexes ?? []).flatMap(i => i.columns))

  // Explicit indexes
  for (const idx of table.indexes ?? []) {
    const cols = idx.columns.join(', ')
    const ops = idx.ops ? ` ${toSqlOps(idx.ops)}` : ''
    lines.push(`-- Why: speed up queries filtering/sorting by [${cols}] on ${name}.\nCREATE INDEX IF NOT EXISTS ${name}_${idx.columns.join('_')}_idx\n  ON ${dbSchema}.${name} USING ${idx.type} (${cols}${ops});`)
  }

  // Auto-indexes for vector columns
  for (const vc of a.vectorCols) {
    if (declared.has(vc.name)) continue
    const method = vc.index ?? 'hnsw'
    lines.push(`-- Why: accelerate nearest-neighbor/vector similarity search on ${name}.${vc.name}.\nCREATE INDEX IF NOT EXISTS ${name}_${vc.name}_idx\n  ON ${dbSchema}.${name} USING ${method} (${vc.name} ${VECTOR_COSINE_OPS_FQN});`)
  }

  // Auto-indexes for jsonb columns
  for (const jc of a.jsonbCols) {
    if (declared.has(jc.name)) continue
    lines.push(`-- Why: speed up JSONB containment/path lookups on ${name}.${jc.name}.\nCREATE INDEX IF NOT EXISTS ${name}_${jc.name}_idx\n  ON ${dbSchema}.${name} USING gin (${jc.name});`)
  }

  // Auto-indexes for array columns
  for (const ac of a.arrayCols) {
    if (declared.has(ac.name)) continue
    lines.push(`-- Why: speed up array overlap/contains queries on ${name}.${ac.name}.\nCREATE INDEX IF NOT EXISTS ${name}_${ac.name}_idx\n  ON ${dbSchema}.${name} USING gin (${ac.name});`)
  }

  return lines.join('\n\n')
}

// â”€â”€ Vector search RPC â”€â”€

function genSearchRpc(name: string, a: TableAnalysis, dbSchema: string): string {
  if (!a.searchVectorCol) return ''
  const vc = a.searchVectorCol
  const dim = vc.type.match(/\d+/)?.[0] ?? '1536'
  const hasCreatedAt = a.columns.some(c => c.name === 'created_at')
  const fallbackOrder = hasCreatedAt ? 't.created_at DESC' : `t.${a.primary.name} DESC`

  const returnCols = a.columns
    .map(c => `  ${c.name} ${toSqlType(c.type)}`)
    .concat(['  similarity float'])
    .join(',\n')

  return `
-- Why: expose one RPC for semantic similarity search plus dynamic metadata filters.
CREATE OR REPLACE FUNCTION ${dbSchema}.search_${name}(
  query_embedding ${VECTOR_TYPE_FQN}(${dim}) DEFAULT NULL,
  match_threshold float DEFAULT 0.0,
  match_count int DEFAULT 10,
  conditions jsonb DEFAULT '[]'::jsonb
)
RETURNS TABLE (
${returnCols}
)
LANGUAGE plpgsql AS $$
DECLARE
  i int;
  cond jsonb;
  f text; o text; v text;
  sql_q text;
  where_parts text[] := ARRAY[]::text[];
BEGIN
  -- Select all columns + computed similarity
  sql_q := 'SELECT t.*';

  IF query_embedding IS NOT NULL THEN
    sql_q := sql_q || ', 1 - (t.${vc.name} OPERATOR(extensions.<=>) $1) AS similarity';
    where_parts := array_append(where_parts,
      '1 - (t.${vc.name} OPERATOR(extensions.<=>) $1) >= ' || match_threshold::text);
  ELSE
    sql_q := sql_q || ', 1.0::float AS similarity';
  END IF;

  sql_q := sql_q || ' FROM ${dbSchema}.${name} t';

  -- Apply dynamic conditions
  FOR i IN 0 .. COALESCE(jsonb_array_length(conditions), 0) - 1 LOOP
    cond := conditions->i;
    f := cond->>'field';
    o := cond->>'op';
    v := cond->>'value';

    CASE o
      WHEN 'eq'       THEN where_parts := array_append(where_parts, format('t.%I = %L', f, v));
      WHEN 'neq'      THEN where_parts := array_append(where_parts, format('t.%I != %L', f, v));
      WHEN 'gt'       THEN where_parts := array_append(where_parts, format('t.%I > %L', f, v));
      WHEN 'gte'      THEN where_parts := array_append(where_parts, format('t.%I >= %L', f, v));
      WHEN 'lt'       THEN where_parts := array_append(where_parts, format('t.%I < %L', f, v));
      WHEN 'lte'      THEN where_parts := array_append(where_parts, format('t.%I <= %L', f, v));
      WHEN 'ilike'    THEN where_parts := array_append(where_parts, format('t.%I ILIKE %L', f, v));
      WHEN 'contains' THEN where_parts := array_append(where_parts, format('t.%I @> %L::jsonb', f, v));
      WHEN 'in'       THEN where_parts := array_append(where_parts,
        format('t.%I = ANY(ARRAY(SELECT jsonb_array_elements_text(%L::jsonb)))', f, v));
      WHEN 'exists'   THEN where_parts := array_append(where_parts, format('t.%I IS NOT NULL', f));
      ELSE NULL;
    END CASE;
  END LOOP;

  IF array_length(where_parts, 1) > 0 THEN
    sql_q := sql_q || ' WHERE ' || array_to_string(where_parts, ' AND ');
  END IF;

  IF query_embedding IS NOT NULL THEN
    sql_q := sql_q || ' ORDER BY t.${vc.name} OPERATOR(extensions.<=>) $1';
  ELSE
    sql_q := sql_q || ' ORDER BY ${fallbackOrder}';
  END IF;

  sql_q := sql_q || ' LIMIT ' || match_count;

  RETURN QUERY EXECUTE sql_q USING query_embedding;
END;
$$;`
}

// â”€â”€ JSONB merge RPC â”€â”€

function genMergeRpc(name: string, col: ResolvedColumn, a: TableAnalysis, dbSchema: string): string {
  const hasUpdatedAt = a.columns.some(c => c.name === 'updated_at')
  const setUpdated = hasUpdatedAt ? ', updated_at = now()' : ''

  return `
-- Why: allow partial JSONB updates without replacing the whole ${col.name} document.
CREATE OR REPLACE FUNCTION ${dbSchema}.update_${name}_${col.name}(
  p_id ${a.primary.type},
  p_data jsonb
)
RETURNS jsonb LANGUAGE plpgsql AS $$
DECLARE
  current_val jsonb;
  merged jsonb;
BEGIN
  SELECT ${col.name} INTO current_val
    FROM ${dbSchema}.${name} WHERE ${a.primary.name} = p_id;

  merged := COALESCE(current_val, '{}'::jsonb) || p_data;

  UPDATE ${dbSchema}.${name}
    SET ${col.name} = merged${setUpdated}
    WHERE ${a.primary.name} = p_id;

  RETURN merged;
END;
$$;`
}

// â”€â”€ JSONB append RPC â”€â”€

function genAppendRpc(name: string, col: ResolvedColumn, a: TableAnalysis, dbSchema: string): string {
  const hasUpdatedAt = a.columns.some(c => c.name === 'updated_at')
  const setUpdated = hasUpdatedAt ? ',\n      updated_at = now()' : ''

  return `
-- Why: append one JSONB entry to ${col.name} while preserving existing array items.
CREATE OR REPLACE FUNCTION ${dbSchema}.append_${name}_${col.name}(
  p_id ${a.primary.type},
  p_entry jsonb
)
RETURNS void LANGUAGE plpgsql AS $$
BEGIN
  UPDATE ${dbSchema}.${name}
    SET ${col.name} = COALESCE(${col.name}, '[]'::jsonb) || jsonb_build_array(p_entry)${setUpdated}
    WHERE ${a.primary.name} = p_id;
END;
$$;`
}

// â”€â”€ Stats aggregation RPC â”€â”€

function genStatsRpc(name: string, col: ResolvedColumn, a: TableAnalysis, dbSchema: string): string {
  return `
-- Why: compute aggregate statistics for numeric metrics stored inside ${col.name}.
CREATE OR REPLACE FUNCTION ${dbSchema}.get_${name}_${col.name}_stats(
  p_metric_key text,
  p_filter jsonb DEFAULT '{}'::jsonb
)
RETURNS TABLE (
  metric_key text, count bigint, avg_value float,
  min_value float, max_value float, stddev_value float
)
LANGUAGE plpgsql AS $$
DECLARE
  sql_q text;
  pair record;
BEGIN
  sql_q := format(
    'SELECT %L::text, count(*)::bigint, avg(val)::float, min(val)::float, max(val)::float, stddev(val)::float '
    'FROM (SELECT (${col.name}->>%L)::float AS val FROM ${dbSchema}.${name} WHERE ${col.name} ? %L',
    p_metric_key, p_metric_key, p_metric_key
  );

  FOR pair IN SELECT * FROM jsonb_each_text(p_filter) LOOP
    sql_q := sql_q || format(' AND %I = %L', pair.key, pair.value);
  END LOOP;

  sql_q := sql_q || ') sub WHERE val IS NOT NULL';

  RETURN QUERY EXECUTE sql_q;
END;
$$;`
}

// â”€â”€ Queue infrastructure (shared, generated once) â”€â”€

function genQueueInfra(dbSchema: string): string {
  const createSchema = dbSchema === 'public' ? '' : `CREATE SCHEMA IF NOT EXISTS ${dbSchema};\n\n`
  return `-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- Shared Queue Infrastructure
-- Generated by code-gen v2
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

${createSchema}
CREATE TABLE IF NOT EXISTS ${dbSchema}.queue_messages (
  id bigserial PRIMARY KEY,
  topic text NOT NULL,
  payload jsonb NOT NULL,
  status text NOT NULL DEFAULT 'ready'
    CHECK (status IN ('ready', 'processing', 'done', 'failed')),
  available_at timestamptz NOT NULL DEFAULT now(),
  locked_at timestamptz,
  locked_by text,
  attempts int NOT NULL DEFAULT 0,
  max_attempts int NOT NULL DEFAULT 5,
  last_error text,
  created_at timestamptz NOT NULL DEFAULT now(),
  updated_at timestamptz NOT NULL DEFAULT now()
);

-- Why: make ready-job polling efficient for workers by topic and availability time.
CREATE INDEX IF NOT EXISTS queue_messages_ready_idx
  ON ${dbSchema}.queue_messages (topic, available_at, id)
  WHERE status = 'ready';

-- Why: centralize insertion of queue jobs and return the created job id.
CREATE OR REPLACE FUNCTION ${dbSchema}.enqueue_message(
  p_topic text, p_payload jsonb, p_available_at timestamptz DEFAULT now()
)
RETURNS bigint LANGUAGE sql AS $$
  INSERT INTO ${dbSchema}.queue_messages (topic, payload, available_at)
  VALUES (p_topic, p_payload, p_available_at) RETURNING id;
$$;

-- Why: atomically lease one ready job for a worker with a visibility timeout.
CREATE OR REPLACE FUNCTION ${dbSchema}.dequeue_message(
  p_topic text, p_worker_id text, visibility_timeout_seconds int DEFAULT 120
)
RETURNS SETOF ${dbSchema}.queue_messages LANGUAGE plpgsql AS $$
DECLARE v_row ${dbSchema}.queue_messages;
BEGIN
  WITH next_job AS (
    SELECT id FROM ${dbSchema}.queue_messages
    WHERE topic = p_topic AND status = 'ready' AND available_at <= now()
    ORDER BY id FOR UPDATE SKIP LOCKED LIMIT 1
  )
  UPDATE ${dbSchema}.queue_messages q SET
    status = CASE WHEN q.attempts + 1 >= q.max_attempts THEN 'failed' ELSE 'processing' END,
    locked_at = now(), locked_by = p_worker_id,
    attempts = q.attempts + 1,
    available_at = now() + make_interval(secs => visibility_timeout_seconds)
  WHERE q.id IN (SELECT id FROM next_job)
  RETURNING q.* INTO v_row;

  IF NOT FOUND THEN RETURN; END IF;
  RETURN NEXT v_row;
END;
$$;

-- Why: mark a leased job as done only for the worker that owns the lock.
CREATE OR REPLACE FUNCTION ${dbSchema}.ack_message(p_id bigint, p_worker_id text)
RETURNS void LANGUAGE sql AS $$
  UPDATE ${dbSchema}.queue_messages
  SET status = 'done', locked_at = null, locked_by = null
  WHERE id = p_id AND locked_by = p_worker_id;
$$;

-- Why: release a failed job back to the queue with retry backoff and error context.
CREATE OR REPLACE FUNCTION ${dbSchema}.nack_message(
  p_id bigint, p_worker_id text,
  backoff_seconds int DEFAULT 30, p_error text DEFAULT null
)
RETURNS void LANGUAGE sql AS $$
  UPDATE ${dbSchema}.queue_messages SET
    status = 'ready', locked_at = null, locked_by = null,
    available_at = now() + make_interval(secs => backoff_seconds),
    last_error = p_error
  WHERE id = p_id AND locked_by = p_worker_id;
$$;
`
}

// â”€â”€ Queue trigger (per table) â”€â”€

function genQueueTrigger(name: string, a: TableAnalysis, dbSchema: string): string {
  if (!a.hasQueueTrigger) return ''

  const contentChecks = a.contentCols.length
    ? a.contentCols.map(c => `(OLD.${c.name} IS DISTINCT FROM NEW.${c.name})`).join('\n       OR ')
    : 'FALSE'

  return `
-- Auto-enqueue trigger for ${name}
-- Why: enqueue background work automatically after inserts/meaningful content updates.
CREATE OR REPLACE FUNCTION ${dbSchema}.enqueue_${name}_job()
RETURNS trigger LANGUAGE plpgsql AS $$
BEGIN
  IF TG_OP = 'INSERT' THEN
    PERFORM ${dbSchema}.enqueue_message(
      '${name}',
      jsonb_build_object('${a.primary.name}', NEW.${a.primary.name})
    );
    RETURN NEW;
  END IF;

  IF TG_OP = 'UPDATE' THEN
    IF ${contentChecks} THEN
      PERFORM ${dbSchema}.enqueue_message(
        '${name}',
        jsonb_build_object('${a.primary.name}', NEW.${a.primary.name})
      );
    END IF;
    RETURN NEW;
  END IF;

  RETURN NEW;
END;
$$;

DROP TRIGGER IF EXISTS ${name}_enqueue_job ON ${dbSchema}.${name};
CREATE TRIGGER ${name}_enqueue_job
  AFTER INSERT OR UPDATE ON ${dbSchema}.${name}
  FOR EACH ROW EXECUTE FUNCTION ${dbSchema}.enqueue_${name}_job();`
}

// â”€â”€ Drop statements (clean re-runs) â”€â”€

function genDrops(name: string, table: TableSchema, a: TableAnalysis, dbSchema: string): string {
  const drops: string[] = []
  const declared = new Set((table.indexes ?? []).flatMap(i => i.columns))
  const indexNames = new Set<string>()

  for (const idx of table.indexes ?? []) {
    indexNames.add(`${name}_${idx.columns.join('_')}_idx`)
  }
  for (const vc of a.vectorCols) {
    if (!declared.has(vc.name)) indexNames.add(`${name}_${vc.name}_idx`)
  }
  for (const jc of a.jsonbCols) {
    if (!declared.has(jc.name)) indexNames.add(`${name}_${jc.name}_idx`)
  }
  for (const ac of a.arrayCols) {
    if (!declared.has(ac.name)) indexNames.add(`${name}_${ac.name}_idx`)
  }

  if (a.hasQueueTrigger) {
    drops.push(`DROP TRIGGER IF EXISTS ${name}_enqueue_job ON ${dbSchema}.${name} CASCADE;`)
    drops.push(`DROP FUNCTION IF EXISTS ${dbSchema}.enqueue_${name}_job CASCADE;`)
  }
  if (a.hasSearch) drops.push(`DROP FUNCTION IF EXISTS ${dbSchema}.search_${name} CASCADE;`)
  for (const c of a.jsonbCols) {
    drops.push(`DROP FUNCTION IF EXISTS ${dbSchema}.update_${name}_${c.name} CASCADE;`)
    drops.push(`DROP FUNCTION IF EXISTS ${dbSchema}.append_${name}_${c.name} CASCADE;`)
    drops.push(`DROP FUNCTION IF EXISTS ${dbSchema}.get_${name}_${c.name}_stats CASCADE;`)
  }
  for (const idx of indexNames) drops.push(`DROP INDEX IF EXISTS ${dbSchema}.${idx} CASCADE;`)
  drops.push(`DROP TABLE IF EXISTS ${dbSchema}.${name} CASCADE;`)

  return drops.join('\n')
}

// â”€â”€ Full SQL composer â”€â”€

function generateSQL(name: string, table: TableSchema, a: TableAnalysis, knownTables: string[] = [], dbSchema = 'public'): string {
  const sections: string[] = []

  sections.push(
    `-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n` +
    `-- ${name} â€” ${table.description ?? 'Table schema and functions'}\n` +
    `-- Generated by code-gen v2\n` +
    `-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`
  )

  const drops = genDrops(name, table, a, dbSchema)
  if (drops) sections.push(drops)
  if (dbSchema !== 'public') {
    sections.push(`CREATE SCHEMA IF NOT EXISTS ${dbSchema};`)
  }

  if (a.hasEmbedder) {
    sections.push(`CREATE SCHEMA IF NOT EXISTS extensions;`)
    sections.push(`CREATE EXTENSION IF NOT EXISTS vector WITH SCHEMA extensions;`)
  }
  if (tableUsesDefaultFn(table, 'gen_random_uuid')) {
    sections.push(`CREATE EXTENSION IF NOT EXISTS pgcrypto;`)
  }
  if (tableUsesDefaultFn(table, 'uuid_generate_v4')) {
    sections.push(`CREATE EXTENSION IF NOT EXISTS "uuid-ossp";`)
  }

  sections.push(genTableDDL(name, table, knownTables, dbSchema))

  const indexes = genIndexes(name, table, a, dbSchema)
  if (indexes) sections.push(indexes)

  if (a.hasSearch) sections.push(genSearchRpc(name, a, dbSchema))
  for (const c of a.mergeCols) sections.push(genMergeRpc(name, c, a, dbSchema))
  for (const c of a.appendCols) sections.push(genAppendRpc(name, c, a, dbSchema))
  for (const c of a.statsCols) sections.push(genStatsRpc(name, c, a, dbSchema))
  if (a.hasQueueTrigger) sections.push(genQueueTrigger(name, a, dbSchema))

  return sections.join('\n\n')
}

function generatePoliciesSQL(name: string, a: TableAnalysis, dbSchema: string): string {
  if (!a.accessRoles.length) return ''

  const roles = a.accessRoles.join(', ')
  const policyName = `${name}_client_access`
  return `-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- ${name} â€” RLS policies
-- Generated by code-gen v2
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ALTER TABLE ${dbSchema}.${name} ENABLE ROW LEVEL SECURITY;

GRANT USAGE ON SCHEMA ${dbSchema} TO ${roles};
GRANT SELECT, INSERT, UPDATE, DELETE ON TABLE ${dbSchema}.${name} TO ${roles};

DROP POLICY IF EXISTS ${policyName} ON ${dbSchema}.${name};
CREATE POLICY ${policyName}
  ON ${dbSchema}.${name}
  FOR ALL
  TO ${roles}
  USING (true)
  WITH CHECK (true);`
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ğŸ“‹ TYPES GENERATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function generateTypes(name: string, _table: TableSchema, a: TableAnalysis): string {
  const P = a.pascal
  const lines: string[] = []

  lines.push(`// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`)
  lines.push(`// ${name} â€” TypeScript types`)
  lines.push(`// Generated by code-gen v2`)
  lines.push(`// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n`)

  // Shared types
  lines.push(`export type ComparisonOp = "eq" | "neq" | "gt" | "gte" | "lt" | "lte" | "between" | "in" | "contains" | "exists" | "ilike";`)
  if (a.hasEmbedder) {
    lines.push(`export type Embedder = { dim: number; embed: (text: string) => Promise<number[]> };`)
  }
  lines.push(`export interface GraphqlEdge<TNode> { node: TNode; cursor: string }`)
  lines.push(`export interface GraphqlPageInfo { hasNextPage: boolean; endCursor: string | null }`)
  lines.push(`export interface GraphqlConnection<TNode> { edges: GraphqlEdge<TNode>[]; pageInfo: GraphqlPageInfo }`)
  lines.push(`export interface GraphqlPageArgs { first?: number; after?: string | null; maxPages?: number }`)
  lines.push(`export interface GraphqlPaginateOptions<TVars extends Record<string, any> = Record<string, any>> extends GraphqlPageArgs { query: string; connectionPath: string; variables?: TVars }`)
  lines.push('')

  // Options interface
  lines.push(`export interface ${P}Options { url: string; key: string; schema?: string; graphqlUrl?: string;${a.hasEmbedder ? ' embedder: Embedder;' : ''} }\n`)

  // Insert interface â€” excludes auto-generated columns
  lines.push(`export interface ${P}Insert {`)
  for (const col of a.columns) {
    if (col.isAutoGenerated) continue
    const opt = col.isOptionalInsert ? '?' : ''
    lines.push(`  ${col.name}${opt}: ${col.tsType};`)
  }
  lines.push(`}\n`)

  // Row interface â€” extends Insert, adds auto-generated + similarity
  lines.push(`export interface ${P}Row extends ${P}Insert {`)
  for (const col of a.columns) {
    if (!col.isAutoGenerated) continue
    const isOptional = col.type.toLowerCase().startsWith('vector') // embeddings can be null
    lines.push(`  ${col.name}${isOptional ? '?' : ''}: ${col.tsType};`)
  }
  if (a.hasSearch) lines.push(`  similarity?: number;`)
  lines.push(`}\n`)

  // Search args
  if (a.hasSearch) {
    lines.push(`export interface ${P}SearchArgs {`)
    lines.push(`  query?: string;`)
    lines.push(`  threshold?: number;`)
    lines.push(`  topK?: number;`)
    lines.push(`  conditions?: { field: string; op: ComparisonOp; value: any }[];`)
    lines.push(`}\n`)
  }

  // Stats
  if (a.statsCols.length) {
    lines.push(`export interface ${P}Stats {`)
    lines.push(`  metric_key: string; count: number; avg_value: number;`)
    lines.push(`  min_value: number; max_value: number; stddev_value: number;`)
    lines.push(`}\n`)
  }

  return lines.join('\n')
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ğŸ”§ SDK GENERATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function generateSDK(name: string, _table: TableSchema, a: TableAnalysis, defaultDbSchema = 'public'): string {
  const P = a.pascal
  const idTs = mapIdType(a.primary.type)
  const L: string[] = [] // lines accumulator

  // Header
  L.push(`// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`)
  L.push(`// ${name} â€” SDK`)
  L.push(`// Generated by code-gen v2`)
  L.push(`// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n`)

  // Imports
  L.push(`import { createClient, type SupabaseClient } from "@supabase/supabase-js";`)
  const typeImports = [`${P}Options`, `${P}Insert`, `${P}Row`, `GraphqlConnection`, `GraphqlPaginateOptions`, `GraphqlPageArgs`]
  if (a.hasEmbedder) typeImports.splice(1, 0, 'Embedder')
  if (a.hasSearch) typeImports.push(`${P}SearchArgs`)
  if (a.statsCols.length) typeImports.push(`${P}Stats`)
  L.push(`import type { ${typeImports.join(', ')} } from "../types/${name}-types";\n`)

  // Class open
  L.push(`export class ${P}Sdk {`)
  L.push(`  private supabase: SupabaseClient;`)
  L.push(`  private apiKey: string;`)
  L.push(`  private graphqlUrl: string;`)
  if (a.hasEmbedder) L.push(`  private embedder: Embedder;`)
  L.push('')

  // â”€â”€ Constructor â”€â”€
  L.push(`  constructor(opts: ${P}Options) {`)
  L.push(`    this.supabase = createClient(opts.url, opts.key, { auth: { persistSession: false, autoRefreshToken: false }, db: { schema: opts.schema ?? "${defaultDbSchema}" } });`)
  L.push(`    this.apiKey = opts.key;`)
  L.push(`    this.graphqlUrl = opts.graphqlUrl ?? \`\${opts.url.replace(/\\/+$/, "")}/graphql/v1\`;`)
  if (a.hasEmbedder) L.push(`    this.embedder = opts.embedder;`)
  L.push(`  }\n`)

  // â”€â”€ GraphQL helpers â”€â”€
  L.push(`  private readPath(input: Record<string, any>, pathExpr: string): any {`)
  L.push(`    return pathExpr.split(".").reduce<any>((acc, key) => (acc == null ? undefined : acc[key]), input);`)
  L.push(`  }\n`)

  L.push(`  private async graphqlRequest<TData>(query: string, variables: Record<string, any> = {}): Promise<TData> {`)
  L.push(`    const response = await fetch(this.graphqlUrl, {`)
  L.push(`      method: "POST",`)
  L.push(`      headers: {`)
  L.push(`        "Content-Type": "application/json",`)
  L.push(`        apikey: this.apiKey,`)
  L.push(`        Authorization: \`Bearer \${this.apiKey}\``)
  L.push(`      },`)
  L.push(`      body: JSON.stringify({ query, variables })`)
  L.push(`    });`)
  L.push(`    if (!response.ok) throw new Error(\`GraphQL request failed: \${response.status} \${response.statusText}\`);`)
  L.push(`    const payload = await response.json() as { data?: TData; errors?: { message?: string }[] };`)
  L.push(`    if (payload.errors?.length) {`)
  L.push(`      const message = payload.errors.map(e => e.message ?? "Unknown GraphQL error").join("; ");`)
  L.push(`      throw new Error(\`GraphQL request failed: \${message}\`);`)
  L.push(`    }`)
  L.push(`    if (!payload.data) throw new Error("GraphQL response missing data");`)
  L.push(`    return payload.data;`)
  L.push(`  }\n`)

  L.push(`  private asConnection<TNode>(data: Record<string, any>, pathExpr: string): GraphqlConnection<TNode> {`)
  L.push(`    const connection = this.readPath(data, pathExpr) as GraphqlConnection<TNode> | undefined;`)
  L.push(`    if (!connection || !Array.isArray(connection.edges) || !connection.pageInfo) {`)
  L.push(`      throw new Error(\`Invalid GraphQL connection at path: \${pathExpr}\`);`)
  L.push(`    }`)
  L.push(`    return connection;`)
  L.push(`  }\n`)

  L.push(`  /**`)
  L.push(`   * Executes a GraphQL connection query and auto-paginates until exhausted or maxPages is reached.`)
  L.push(`   * Query must support variables: $first: Int and $after: Cursor.`)
  L.push(`   */`)
  L.push(`  async paginateGraphqlQuery<TNode, TVars extends Record<string, any> = Record<string, any>>(opts: GraphqlPaginateOptions<TVars>): Promise<TNode[]> {`)
  L.push(`    const first = opts.first ?? 50;`)
  L.push(`    const maxPages = opts.maxPages ?? 100;`)
  L.push(`    let after = opts.after ?? null;`)
  L.push(`    const results: TNode[] = [];`)
  L.push(`    for (let page = 0; page < maxPages; page += 1) {`)
  L.push(`      const variables: Record<string, any> = { ...(opts.variables ?? {}), first, after };`)
  L.push(`      const data = await this.graphqlRequest<Record<string, any>>(opts.query, variables);`)
  L.push(`      const connection = this.asConnection<TNode>(data, opts.connectionPath);`)
  L.push(`      results.push(...connection.edges.map(edge => edge.node));`)
  L.push(`      if (!connection.pageInfo.hasNextPage) break;`)
  L.push(`      after = connection.pageInfo.endCursor;`)
  L.push(`      if (!after) break;`)
  L.push(`    }`)
  L.push(`    return results;`)
  L.push(`  }\n`)

  L.push(`  /** Fetch a single GraphQL page from ${name}Collection. */`)
  L.push(`  async listPageGraphql(args: GraphqlPageArgs = {}): Promise<GraphqlConnection<${P}Row>> {`)
  L.push(`    const query = \`query ${P}Page($first: Int = 50, $after: Cursor) {`)
  L.push(`      ${name}Collection(first: $first, after: $after) {`)
  L.push(`        edges {`)
  L.push(`          node {`)
  for (const col of a.columns) {
    L.push(`            ${col.name}`)
  }
  L.push(`          }`)
  L.push(`          cursor`)
  L.push(`        }`)
  L.push(`        pageInfo { hasNextPage endCursor }`)
  L.push(`      }`)
  L.push(`    }\`;`)
  L.push(`    const data = await this.graphqlRequest<Record<string, any>>(query, { first: args.first ?? 50, after: args.after ?? null });`)
  L.push(`    return this.asConnection<${P}Row>(data, "${name}Collection");`)
  L.push(`  }\n`)

  L.push(`  /** Fetch and flatten all pages from ${name}Collection. */`)
  L.push(`  async listAllGraphql(args: GraphqlPageArgs = {}): Promise<${P}Row[]> {`)
  L.push(`    const query = \`query ${P}Page($first: Int = 50, $after: Cursor) {`)
  L.push(`      ${name}Collection(first: $first, after: $after) {`)
  L.push(`        edges {`)
  L.push(`          node {`)
  for (const col of a.columns) {
    L.push(`            ${col.name}`)
  }
  L.push(`          }`)
  L.push(`          cursor`)
  L.push(`        }`)
  L.push(`        pageInfo { hasNextPage endCursor }`)
  L.push(`      }`)
  L.push(`    }\`;`)
  L.push(`    return this.paginateGraphqlQuery<${P}Row>({`)
  L.push(`      query,`)
  L.push(`      connectionPath: "${name}Collection",`)
  L.push(`      first: args.first ?? 50,`)
  L.push(`      after: args.after ?? null,`)
  L.push(`      maxPages: args.maxPages ?? 100`)
  L.push(`    });`)
  L.push(`  }\n`)

  // â”€â”€ insert (insert + auto-embed) â”€â”€
  const embedEntries = Array.from(a.embedMap.entries()) // [[vecCol, srcCol], ...]
  L.push(`  /** Insert a record${embedEntries.length ? ' (auto-embeds source fields)' : ''} */`)
  L.push(`  async insert(data: ${P}Insert): Promise<${P}Row> {`)
  for (const [vecCol, srcCol] of embedEntries) {
    L.push(`    const ${toCamel(vecCol)} = data.${srcCol} ? await this.embedder.embed(data.${srcCol}) : null;`)
  }
  if (embedEntries.length) {
    const spread = embedEntries.map(([vc]) => `${vc}: ${toCamel(vc)}`).join(', ')
    L.push(`    const { data: row, error } = await this.supabase.from("${name}").insert({ ...data, ${spread} }).select("*").single();`)
  } else {
    L.push(`    const { data: row, error } = await this.supabase.from("${name}").insert(data).select("*").single();`)
  }
  L.push(`    if (error) throw new Error(\`Insert ${name} failed: \${error.message}\`);`)
  L.push(`    return row;`)
  L.push(`  }\n`)
  L.push(`  /** @deprecated Use insert(...) instead. */`)
  L.push(`  async log(data: ${P}Insert): Promise<${P}Row> {`)
  L.push(`    return this.insert(data);`)
  L.push(`  }\n`)

  // â”€â”€ get â”€â”€
  L.push(`  /** Retrieve a single record by ID */`)
  L.push(`  async get(id: ${idTs}): Promise<${P}Row | null> {`)
  L.push(`    const { data, error } = await this.supabase.from("${name}").select("*").eq("${a.primary.name}", id).single();`)
  L.push(`    if (error) return null;`)
  L.push(`    return data;`)
  L.push(`  }\n`)

  // â”€â”€ update â”€â”€
  L.push(`  /** Update a record by ID */`)
  L.push(`  async update(id: ${idTs}, data: Partial<${P}Insert>): Promise<${P}Row> {`)
  if (embedEntries.length) {
    L.push(`    const updateData: Record<string, any> = { ...data };`)
    for (const [vecCol, srcCol] of embedEntries) {
      L.push(`    if (data.${srcCol} !== undefined) updateData.${vecCol} = data.${srcCol} ? await this.embedder.embed(data.${srcCol}) : null;`)
    }
    L.push(`    const { data: row, error } = await this.supabase.from("${name}").update(updateData).eq("${a.primary.name}", id).select("*").single();`)
  } else {
    L.push(`    const { data: row, error } = await this.supabase.from("${name}").update(data).eq("${a.primary.name}", id).select("*").single();`)
  }
  L.push(`    if (error) throw new Error(\`Update ${name} failed: \${error.message}\`);`)
  L.push(`    return row;`)
  L.push(`  }\n`)

  // â”€â”€ delete â”€â”€
  L.push(`  /** Delete a record by ID */`)
  L.push(`  async delete(id: ${idTs}): Promise<void> {`)
  L.push(`    const { error } = await this.supabase.from("${name}").delete().eq("${a.primary.name}", id);`)
  L.push(`    if (error) throw new Error(\`Delete ${name} failed: \${error.message}\`);`)
  L.push(`  }\n`)

  // â”€â”€ search (hybrid: vector + conditions) â”€â”€
  if (a.hasSearch) {
    L.push(`  /** Hybrid search: semantic similarity + metadata/field conditions */`)
    L.push(`  async search(args: ${P}SearchArgs): Promise<${P}Row[]> {`)
    L.push(`    const embedding = args.query ? await this.embedder.embed(args.query) : null;`)
    L.push(`    const { data, error } = await this.supabase.rpc("search_${name}", {`)
    L.push(`      query_embedding: embedding,`)
    L.push(`      match_threshold: args.threshold ?? 0.1,`)
    L.push(`      match_count: args.topK ?? 10,`)
    L.push(`      conditions: args.conditions ?? []`)
    L.push(`    });`)
    L.push(`    if (error) throw new Error(\`Search ${name} failed: \${error.message}\`);`)
    L.push(`    return data ?? [];`)
    L.push(`  }\n`)
  }

  // â”€â”€ JSONB merge methods â”€â”€
  for (const mc of a.mergeCols) {
    L.push(`  /** Merge data into ${mc.name} (non-destructive JSONB update via RPC) */`)
    L.push(`  async update${toPascal(mc.name)}(id: ${idTs}, data: Record<string, any>): Promise<Record<string, any>> {`)
    L.push(`    let { data: result, error } = await this.supabase.rpc("update_${name}_${mc.name}", { p_id: id, p_data: data });`)
    L.push(`    if (error?.message?.includes("Could not find the function")) {`)
    L.push(`      const retry = await this.supabase.rpc("update_${name}_${mc.name}", { id, data });`)
    L.push(`      result = retry.data;`)
    L.push(`      error = retry.error;`)
    L.push(`    }`)
    L.push(`    if (error) throw new Error(\`Update ${mc.name} failed: \${error.message}\`);`)
    L.push(`    return result;`)
    L.push(`  }\n`)
  }

  // â”€â”€ JSONB append methods â”€â”€
  for (const ac of a.appendCols) {
    L.push(`  /** Append entry to ${ac.name} (JSONB array append via RPC) */`)
    L.push(`  async append${toPascal(ac.name)}(id: ${idTs}, entry: any): Promise<void> {`)
    L.push(`    const { error } = await this.supabase.rpc("append_${name}_${ac.name}", { p_id: id, p_entry: entry });`)
    L.push(`    if (error) throw new Error(\`Append ${ac.name} failed: \${error.message}\`);`)
    L.push(`  }\n`)
  }

  // â”€â”€ Stats methods â”€â”€
  for (const sc of a.statsCols) {
    L.push(`  /** Aggregate stats for a metric key in ${sc.name} */`)
    L.push(`  async ${toCamel(sc.name)}Stats(metricKey: string, filter: Record<string, any> = {}): Promise<${P}Stats | null> {`)
    L.push(`    const { data, error } = await this.supabase.rpc("get_${name}_${sc.name}_stats", { p_metric_key: metricKey, p_filter: filter });`)
    L.push(`    if (error) throw new Error(\`Get ${sc.name} stats failed: \${error.message}\`);`)
    L.push(`    return data?.[0] ?? null;`)
    L.push(`  }\n`)
  }

  // â”€â”€ Array filter methods â”€â”€
  for (const ac of a.arrayCols) {
    const elemType = mapTsType(ac.type.replace('[]', ''))

    if (ac.resolvedPatterns.includes('array_overlap')) {
      L.push(`  /** Find records where ${ac.name} overlaps with any of the given values */`)
      L.push(`  async filterBy${toPascal(ac.name)}Overlaps(values: ${elemType}[]): Promise<${P}Row[]> {`)
      L.push(`    const { data, error } = await this.supabase.from("${name}").select("*").overlaps("${ac.name}", values);`)
      L.push(`    if (error) throw new Error(\`Filter ${ac.name} overlaps failed: \${error.message}\`);`)
      L.push(`    return data ?? [];`)
      L.push(`  }\n`)
    }

    if (ac.resolvedPatterns.includes('array_contains')) {
      L.push(`  /** Find records where ${ac.name} contains all given values */`)
      L.push(`  async filterBy${toPascal(ac.name)}Contains(values: ${elemType}[]): Promise<${P}Row[]> {`)
      L.push(`    const { data, error } = await this.supabase.from("${name}").select("*").contains("${ac.name}", values);`)
      L.push(`    if (error) throw new Error(\`Filter ${ac.name} contains failed: \${error.message}\`);`)
      L.push(`    return data ?? [];`)
      L.push(`  }\n`)
    }
  }

  // Class close
  L.push(`}`)
  L.push('')

  return L.join('\n')
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ğŸ§ª EXAMPLE GENERATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function sampleValueForTsType(tsType: string, pgType?: string): string {
  if (pgType?.toLowerCase().replace(/\s+/g, '') === 'uuid') return `crypto.randomUUID()`
  if (tsType.endsWith('[]')) {
    const itemType = tsType.slice(0, -2)
    return `[${sampleValueForTsType(itemType)}]`
  }
  if (tsType === 'string') return `"example_value"`
  if (tsType === 'number') return `1`
  if (tsType === 'boolean') return `true`
  if (tsType === 'Record<string, any>' || tsType === 'any') return `{ sample: true }`
  return `null`
}

function generateExampleFile(schema: Schema, tables: Array<{ name: string; analysis: TableAnalysis }>, defaultDbSchema = 'public'): string {
  const lines: string[] = []
  const hasEmbedder = tables.some(t => t.analysis.hasEmbedder)
  const embedDim = schema.config?.embed_dim ?? 1536

  lines.push(`import "dotenv/config";`)
  for (const t of tables) {
    lines.push(`import { ${t.analysis.pascal}Sdk } from "./sdk/${t.name}-sdk";`)
  }
  lines.push('')
  lines.push(`/** ---------------- Config ---------------- */`)
  lines.push(`const SUPABASE_URL = process.env.SUPABASE_URL ?? "";`)
  lines.push(`const SUPABASE_KEY = process.env.SUPABASE_KEY ?? "";`)
  lines.push(`const SUPABASE_DB_SCHEMA = process.env.SUPABASE_DB_SCHEMA ?? "${defaultDbSchema}";`)

  if (hasEmbedder) {
    lines.push(`const EMBEDDING_DIMENSIONS = ${embedDim};`)
    lines.push('')
    lines.push(`/** Stub embedder: returns a zero vector of the configured dimension */`)
    lines.push(`const embedder = {`)
    lines.push(`  dim: EMBEDDING_DIMENSIONS,`)
    lines.push(`  async embed(_text: string): Promise<number[]> {`)
    lines.push(`    return Array.from({ length: EMBEDDING_DIMENSIONS }, () => 0);`)
    lines.push(`  }`)
    lines.push(`};`)
  }

  for (const t of tables) {
    const a = t.analysis
    const camel = toCamel(a.pascal)
    const fnName = `run${a.pascal}Examples`
    const insertCols = a.columns.filter(c => !c.isAutoGenerated && !c.isOptionalInsert)
    const updateCol = a.columns.find(c => !c.isAutoGenerated && !c.primary) ?? null
    const idAccess = `created.${a.primary.name}`
    const queryCols = a.columns.slice(0, 3).map(c => c.name).join(' ')
    const conditionCol = a.columns.find(c =>
      !c.primary &&
      !c.type.toLowerCase().startsWith('vector') &&
      !['jsonb', 'json'].includes(c.type.toLowerCase()) &&
      !c.type.endsWith('[]')
    )

    lines.push('')
    lines.push(`/** ---------------- Sample Data ---------------- */`)
    lines.push(`const ${camel}InsertSampleData = {`)
    for (const col of insertCols) {
      lines.push(`  ${col.name}: ${sampleValueForTsType(col.tsType, col.type)},`)
    }
    lines.push(`};`)
    lines.push('')
    lines.push(`/** Cursor-based pagination query: fetches ${queryCols} per edge */`)
    lines.push(`const ${camel}PageSampleQuery = \`query ${a.pascal}Example($first: Int = 2, $after: Cursor) {`)
    lines.push(`  ${t.name}Collection(first: $first, after: $after) {`)
    lines.push(`    edges { node { ${queryCols} } cursor }`)
    lines.push(`    pageInfo { hasNextPage endCursor }`)
    lines.push(`  }`)
    lines.push(`}\`;`)
    if (a.hasSearch) {
      lines.push('')
      lines.push(`const ${camel}SearchSampleData = {`)
      lines.push(`  query: "example query",`)
      lines.push(`  threshold: 0.1,`)
      lines.push(`  topK: 5,`)
      if (conditionCol) {
        lines.push(`  conditions: [{ field: "${conditionCol.name}", op: "eq" as const, value: ${sampleValueForTsType(conditionCol.tsType, conditionCol.type)} }],`)
      }
      lines.push(`};`)
    }

    lines.push('')
    lines.push(`/** ---------------- Examples ---------------- */`)
    lines.push(`async function ${fnName}(): Promise<void> {`)
    if (a.hasEmbedder) {
      lines.push(`  const sdk = new ${a.pascal}Sdk({ url: SUPABASE_URL, key: SUPABASE_KEY, schema: SUPABASE_DB_SCHEMA, embedder });`)
    } else {
      lines.push(`  const sdk = new ${a.pascal}Sdk({ url: SUPABASE_URL, key: SUPABASE_KEY, schema: SUPABASE_DB_SCHEMA });`)
    }
    lines.push('')
    lines.push(`  /** Insert a new row and log its id */`)
    lines.push(`  const created = await sdk.insert(${camel}InsertSampleData);`)
    lines.push(`  console.log("[${t.name}] created:", created.${a.primary.name});`)
    lines.push(``)
    lines.push(`  /** Fetch the row by id */`)
    lines.push(`  const fetched = await sdk.get(${idAccess});`)
    lines.push(`  console.log("[${t.name}] fetched:", fetched ? "found" : "not found");`)
    lines.push(``)
    lines.push(`  /** Update a single field on the row */`)
    lines.push(`  const updated = await sdk.update(${idAccess}, ${updateCol ? `{ ${updateCol.name}: ${sampleValueForTsType(updateCol.tsType, updateCol.type)} }` : '{}'});`)
    lines.push(`  console.log("[${t.name}] updated:", updated.${a.primary.name});`)
    lines.push(``)
    lines.push(`  /** Fetch one page of results (limit: 2) */`)
    lines.push(`  const page = await sdk.listPageGraphql({ first: 2 });`)
    lines.push(`  console.log("[${t.name}] page edges:", page.edges.length);`)
    lines.push(``)
    lines.push(`  /** Fetch all rows using auto-pagination (capped at 1 page for this example) */`)
    lines.push(`  const allRows = await sdk.listAllGraphql({ first: 2, maxPages: 1 });`)
    lines.push(`  console.log("[${t.name}] all rows:", allRows.length);`)
    lines.push(``)
    lines.push(`  /** Run a raw GraphQL query with auto-pagination */`)
    lines.push(`  const paginated = await sdk.paginateGraphqlQuery({ query: ${camel}PageSampleQuery, connectionPath: "${t.name}Collection", first: 2, maxPages: 1 });`)
    lines.push(`  console.log("[${t.name}] paginated rows:", paginated.length);`)

    if (a.hasSearch) {
      lines.push(``)
      lines.push(`  /** Vector search with a similarity threshold and optional field filters */`)
      lines.push(`  const searchRows = await sdk.search(${camel}SearchSampleData);`)
      lines.push(`  console.log("[${t.name}] search rows:", searchRows.length);`)
    }

    for (const mc of a.mergeCols) {
      lines.push(``)
      lines.push(`  /** Merge data into the \`${mc.name}\` JSONB column via RPC; skip if the function doesn't exist */`)
      lines.push(`  try {`)
      lines.push(`    await sdk.update${toPascal(mc.name)}(${idAccess}, { merged: true });`)
      lines.push(`    console.log("[${t.name}] update${toPascal(mc.name)}: ok");`)
      lines.push(`  } catch (error) {`)
      lines.push(`    if (error instanceof Error && error.message.includes("Could not find the function")) {`)
      lines.push(`      console.log("[${t.name}] update${toPascal(mc.name)}: skipped (RPC not found) Details:", error.message);`)
      lines.push(`    } else {`)
      lines.push(`      throw error;`)
      lines.push(`    }`)
      lines.push(`  }`)
    }
    for (const ac of a.appendCols) {
      lines.push(``)
      lines.push(`  /** Append an entry to the \`${ac.name}\` JSONB array via RPC; skip if the function doesn't exist */`)
      lines.push(`  try {`)
      lines.push(`    await sdk.append${toPascal(ac.name)}(${idAccess}, { event: "example" });`)
      lines.push(`    console.log("[${t.name}] append${toPascal(ac.name)}: ok");`)
      lines.push(`  } catch (error) {`)
      lines.push(`    if (error instanceof Error && error.message.includes("Could not find the function")) {`)
      lines.push(`      console.log("[${t.name}] append${toPascal(ac.name)}: skipped (RPC not found) Details:", error.message);`)
      lines.push(`    } else {`)
      lines.push(`      throw error;`)
      lines.push(`    }`)
      lines.push(`  }`)
    }
    for (const sc of a.statsCols) {
      lines.push(``)
      lines.push(`  /** Fetch stats from \`${sc.name}\` via RPC; skip if the function doesn't exist */`)
      lines.push(`  try {`)
      lines.push(`    const stats = await sdk.${toCamel(sc.name)}Stats("score", {});`)
      lines.push(`    console.log("[${t.name}] ${toCamel(sc.name)}Stats:", Boolean(stats));`)
      lines.push(`  } catch (error) {`)
      lines.push(`    if (error instanceof Error && error.message.includes("Could not find the function")) {`)
      lines.push(`      console.log("[${t.name}] ${toCamel(sc.name)}Stats: skipped (RPC not found) Details:", error.message);`)
      lines.push(`    } else {`)
      lines.push(`      throw error;`)
      lines.push(`    }`)
      lines.push(`  }`)
    }
    for (const ac of a.arrayCols) {
      const sampleArray = sampleValueForTsType(mapTsType(ac.type))
      if (ac.resolvedPatterns.includes('array_overlap')) {
        lines.push(``)
        lines.push(`  /** Filter rows where the \`${ac.name}\` array overlaps with the given list (any match) */`)
        lines.push(`  const overlapRows = await sdk.filterBy${toPascal(ac.name)}Overlaps(${sampleArray});`)
        lines.push(`  console.log("[${t.name}] filterBy${toPascal(ac.name)}Overlaps rows:", overlapRows.length);`)
      }
      if (ac.resolvedPatterns.includes('array_contains')) {
        lines.push(``)
        lines.push(`  /** Filter rows where the \`${ac.name}\` array contains all values in the given list */`)
        lines.push(`  const containsRows = await sdk.filterBy${toPascal(ac.name)}Contains(${sampleArray});`)
        lines.push(`  console.log("[${t.name}] filterBy${toPascal(ac.name)}Contains rows:", containsRows.length);`)
      }
    }

    lines.push(``)
    lines.push(`  /** Clean up: delete the row created at the start */`)
    lines.push(`  await sdk.delete(${idAccess});`)
    lines.push(`  console.log("[${t.name}] deleted:", created.${a.primary.name});`)
    lines.push(`}`)
  }

  lines.push('')
  lines.push(`/** ---------------- Entry Point ---------------- */`)
  lines.push(`async function main(): Promise<void> {`)
  lines.push(`  if (!SUPABASE_URL || !SUPABASE_KEY) throw new Error("Missing SUPABASE_URL or SUPABASE_KEY");`)
  for (const t of tables) {
  lines.push(`  await run${t.analysis.pascal}Examples();`)
  }
  lines.push(`}`)
  lines.push('')
  lines.push(`main().catch((error) => {`)
  lines.push(`  console.error(error);`)
  lines.push(`  process.exit(1);`)
  lines.push(`});`)
  lines.push('')

  return lines.join('\n')
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ğŸš€ Orchestrator + CLI
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async function generate(schemaPath: string, outputDir: string): Promise<void> {
  const raw = fs.readFileSync(schemaPath, 'utf8')
  const schema: Schema = yaml.parse(raw)
  const dir = schema.config?.output_dir ?? outputDir
  const dbSchema = normalizeDbSchema(schema.config?.db_schema)
  const tableNames = Object.keys(schema.tables)



  if (!fs.existsSync(dir)) fs.mkdirSync(dir, { recursive: true })


  const writeFile = async (path: string, content: string) => {
    await fs.promises.mkdir(path.split('/').slice(0, -1).join('/'), { recursive: true })
    await fs.promises.writeFile(path, content)
  }

  let needsQueue = false
  const analyzedTables: Array<{ name: string; analysis: TableAnalysis }> = []

  for (const [name, table] of Object.entries(schema.tables)) {
    const analysis = analyzeTable(name, table)
    analyzedTables.push({ name, analysis })
    if (analysis.hasQueueTrigger) needsQueue = true

    // SQL migration
    const sql = generateSQL(name, table, analysis, tableNames, dbSchema)
    await writeFile(path.join(dir, `sql/${name}.sql`), sql)
    console.log(`  âœ“ ${name}.sql`)

    // TypeScript types
    const types = generateTypes(name, table, analysis)
    await writeFile(path.join(dir, `types/${name}-types.ts`), types)
    console.log(`  âœ“ ${name}-types.ts`)

    // TypeScript SDK
    const sdk = generateSDK(name, table, analysis, dbSchema)
    await writeFile(path.join(dir, `sdk/${name}-sdk.ts`), sdk)
    console.log(`  âœ“ ${name}-sdk.ts`)
  }

  const example = generateExampleFile(schema, analyzedTables, dbSchema)
  await writeFile(path.join(dir, 'example.ts'), example)
  console.log(`  âœ“ example.ts`)

  // Shared queue infrastructure (generated once if any table needs it)
  if (needsQueue) {
    await writeFile(path.join(dir, 'queue.sql'), genQueueInfra(dbSchema))
    console.log(`  âœ“ queue.sql (shared queue infrastructure)`)
  }

  console.log(`\nâœ… Generated ${Object.keys(schema.tables).length} table(s) â†’ ${dir}/`)
}

// â”€â”€ CLI entry â”€â”€
const args = process.argv.slice(2)
const schemaPath = args[0] || 'schema.yaml'
const outputDir = args[1] || './generated'

async function runCli(): Promise<void> {
  try {
    console.log(`\nğŸ”§ Supabase Code Generator v2`)
    console.log(`   Schema: ${schemaPath}`)
    console.log(`   Output: ${outputDir}\n`)
    await generate(schemaPath, outputDir)
  } catch (err) {
    console.error('âŒ Generation failed:', err)
    process.exit(1)
  }
}

runCli()

export { generate, analyzeTable, generateSQL, generateTypes, generateSDK, generateExampleFile }
